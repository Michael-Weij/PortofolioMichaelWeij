{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c68d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import threading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from ipywidgets import widgets\n",
    "from scipy import stats\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "# Config\n",
    "surrounding_duration = timedelta(days=15)\n",
    "\n",
    "# Internal globals\n",
    "cached_donors = None\n",
    "donors = None\n",
    "gaps_done = 0\n",
    "column_name = None\n",
    "running = True\n",
    "imputing_classification = False\n",
    "hd_donors_selector = None\n",
    "hd_confirm_button = None\n",
    "\n",
    "\n",
    "def hotdeck_classification(receiver: pd.DataFrame, config: object) -> Tuple[pd.DataFrame, str]:\n",
    "    global imputing_classification\n",
    "    imputing_classification = True\n",
    "    imputed, imputer_config = hotdeck(receiver, config)\n",
    "    imputed[column_name] = imputed[column_name].round()\n",
    "    return imputed, imputer_config\n",
    "\n",
    "\n",
    "def hotdeck(receiver: pd.DataFrame, config: object) -> Tuple[pd.DataFrame, str]:\n",
    "    global file_select, donors, gaps_done, running\n",
    "\n",
    "    custom_progress_status = widgets.HTML()\n",
    "    display(custom_progress_status)\n",
    "\n",
    "    if cached_donors == None:\n",
    "        load_donors_in_cache(donors, custom_progress_status)\n",
    "\n",
    "    gaps_done = 0\n",
    "    receiver_cp = receiver.copy()\n",
    "    gap_indices = config[\"current_gap_indices\"]\n",
    "\n",
    "    ui_thread = threading.Thread(target=run_ui, args=(len(gap_indices), custom_progress_status))\n",
    "    ui_thread.start()\n",
    "    try:\n",
    "        impute_gaps(receiver_cp, gap_indices)\n",
    "    except:\n",
    "        running = False\n",
    "        ui_thread.join()\n",
    "        raise\n",
    "    ui_thread.join()\n",
    "\n",
    "    return receiver_cp, json.dumps({\n",
    "        'donors': donors,\n",
    "        'surrounding_duration': str(surrounding_duration)\n",
    "    })\n",
    "\n",
    "\n",
    "def impute_gaps(receiver: pd.DataFrame, gap_indices: [[datetime]]) -> None:\n",
    "    global surrounding_duration, column_name, donors, gaps_done\n",
    "\n",
    "    for gap in gap_indices:\n",
    "        gap_start_idx, gap_end_idx = get_gap_boundaries(receiver, gap[0], gap[-1])\n",
    "        gap_start_time = receiver.index[gap_start_idx]\n",
    "        gap_end_time = receiver.index[gap_end_idx]\n",
    "\n",
    "        duration_before, duration_after = get_sampling_durations(\n",
    "            receiver, gap_start_idx, gap_end_idx, gap_start_time, gap_end_time)\n",
    "\n",
    "        before = get_normalized_dataframe(receiver, gap_start_time - duration_before, gap_start_time)\n",
    "        after = get_normalized_dataframe(receiver, gap_end_time, gap_end_time + duration_after)\n",
    "\n",
    "        donor_start_time = gap_start_time - (duration_before + surrounding_duration)\n",
    "        donor_end_time = gap_end_time + (duration_after + surrounding_duration)\n",
    "\n",
    "        scoreboard = []\n",
    "        for file in donors:\n",
    "            donor = get_donor(file, donor_start_time, donor_end_time)\n",
    "            if len(donor.index) != 0:\n",
    "                try:\n",
    "                    res = scan_donor(before.copy(), after.copy(), file, donor)\n",
    "                    scoreboard.append(res)\n",
    "                except:\n",
    "                    pass\n",
    "        if len(scoreboard) != 0:\n",
    "            scoreboard.sort(key=lambda it: it[\"score\"])\n",
    "            impute_gap(scoreboard[0], receiver, gap)\n",
    "        gaps_done += 1\n",
    "    receiver.interpolate(method=\"time\", limit_direction=\"both\", inplace=True, downcast='infer')\n",
    "\n",
    "\n",
    "def scan_donor(before: pd.DataFrame, after: pd.DataFrame, donor_filename: str, donor: pd.DataFrame) -> [dict]:\n",
    "    global column_name, imputing_classification\n",
    "\n",
    "    keys = np.concatenate([before[column_name].values, after[column_name].values])\n",
    "\n",
    "    gap_size = after.index[0] - before.index[-1]\n",
    "    before_size = before.index[-1] - before.index[0]\n",
    "    after_size = after.index[-1] - after.index[0]\n",
    "\n",
    "    donor_before = get_normalized_dataframe(donor, donor.index[0], donor.index[-1] - (after_size + gap_size))\n",
    "    donor_after = get_normalized_dataframe(donor, donor.index[0] + (gap_size + before_size), donor.index[-1])\n",
    "\n",
    "    sliding_before = sliding_window_view(donor_before[column_name].values, window_shape=len(before.index))\n",
    "    sliding_after = sliding_window_view(donor_after[column_name].values, window_shape=len(after.index))\n",
    "\n",
    "    length = min(len(sliding_before), len(sliding_after))\n",
    "    matrix = np.concatenate([sliding_before[:length], sliding_after[:length]], axis=1)\n",
    "\n",
    "    if not imputing_classification:\n",
    "        y_offsets = keys.mean() - matrix.mean(axis=1)\n",
    "        matrix = np.array([matrix[i] + y_offsets[i] for i in range(len(y_offsets))])\n",
    "    else:\n",
    "        y_offsets = np.zeros(len(matrix))\n",
    "        matrix = matrix.astype(np.float64)\n",
    "\n",
    "    matrix -= keys\n",
    "    matrix = np.absolute(matrix)\n",
    "\n",
    "    comp = matrix.sum(axis=1)\n",
    "    best = np.argsort(comp)[0]\n",
    "\n",
    "    return {\n",
    "        \"score\": comp[best],\n",
    "        \"x_offset\": before.index[0] - donor.index[best],\n",
    "        \"y_offset\": y_offsets[best],\n",
    "        \"donor\": donor\n",
    "    }\n",
    "\n",
    "\n",
    "def impute_gap(best: dict, receiver: pd.DataFrame, gap_indices: [datetime]) -> None:\n",
    "    global column_name, imputing_classification\n",
    "    donor = best[\"donor\"].copy()\n",
    "    donor.index += best[\"x_offset\"]\n",
    "    donor[column_name] += best[\"y_offset\"]\n",
    "    if not imputing_classification:\n",
    "        donor = donor[\n",
    "            (donor[column_name] > donor[column_name].quantile(0.05)) &\n",
    "            (donor[column_name] < donor[column_name].quantile(0.95))\n",
    "        ]\n",
    "    for gap_idx in gap_indices:\n",
    "        if gap_idx not in donor.index:\n",
    "            donor.loc[gap_idx] = np.nan\n",
    "    donor.sort_index(key=lambda t: t.astype(np.int64), inplace=True)\n",
    "    donor.interpolate(method=\"time\", limit_direction=\"both\", inplace=True, downcast='infer')\n",
    "    for gap_idx in gap_indices:\n",
    "        receiver[column_name][gap_idx] = donor[column_name][gap_idx]\n",
    "\n",
    "\n",
    "def hotdeck_prehook(next_step):\n",
    "    global file_select, dfloader, column_name, hd_donors_selector, hd_confirm_button\n",
    "\n",
    "    column_name = dfloader.targets[0]\n",
    "\n",
    "    if cached_donors is not None:\n",
    "        next_step()\n",
    "        return\n",
    "\n",
    "    files = filter_compatible_files(file_select.options)\n",
    "    hd_donors_selector = widgets.SelectMultiple(\n",
    "        options=files,\n",
    "        description=\"Select donors: \",\n",
    "        rows=len(files)\n",
    "    )\n",
    "    hd_confirm_button = widgets.Button(description=\"Confirm selection\")\n",
    "    display(hd_donors_selector)\n",
    "    display(hd_confirm_button)\n",
    "\n",
    "    def callback(e):\n",
    "        global donors, hd_confirm_button, hd_donors_selector\n",
    "        hd_confirm_button.close()\n",
    "        hd_donors_selector.disabled = True\n",
    "        donors = hd_donors_selector.value\n",
    "        next_step()\n",
    "    hd_confirm_button.on_click(callback)\n",
    "\n",
    "\n",
    "def filter_compatible_files(files: [str]) -> [str]:\n",
    "    global dfloader\n",
    "    sheet_name = dfloader.targets_index[0][0]\n",
    "    return [file for file in files if \".csv\" in file or sheet_name in pd.ExcelFile(config[\"upload_dir\"] + file, engine='openpyxl').sheet_names]\n",
    "\n",
    "\n",
    "def get_normalized_dataframe(df: pd.DataFrame, start_time: datetime, end_time: datetime) -> pd.DataFrame:\n",
    "    start_idx = df.index.get_loc(start_time if start_time >= df.index[0] else df.index[0], 'pad')\n",
    "    end_idx = df.index.get_loc(end_time, 'bfill')\n",
    "    cp = df[(df.index >= df.index[start_idx]) & (df.index <= df.index[end_idx])].copy()\n",
    "\n",
    "    start_time_missing = cp.index[0] != start_time\n",
    "    end_time_missing = cp.index[-1] != end_time\n",
    "\n",
    "    if start_time_missing or end_time_missing:\n",
    "        if start_time_missing:\n",
    "            cp.loc[start_time] = np.nan\n",
    "        if end_time_missing:\n",
    "            cp.loc[end_time] = np.nan\n",
    "        cp.sort_index(key=lambda t: t.astype(np.int64), inplace=True)\n",
    "        cp.interpolate(method=\"time\", limit_direction=\"both\", inplace=True, downcast='infer')\n",
    "    return cp[(cp.index >= start_time) & (cp.index <= end_time)]\n",
    "\n",
    "\n",
    "def get_donor(filepath: str, start_time: datetime = None, end_time: datetime = None) -> None:\n",
    "    global cached_donors\n",
    "    donor = cached_donors[filepath].copy()\n",
    "    if start_time != None:\n",
    "        donor = donor[donor.index >= start_time]\n",
    "    if end_time != None:\n",
    "        donor = donor[donor.index <= end_time]\n",
    "    return donor\n",
    "\n",
    "\n",
    "def load_donors_in_cache(donors: [str], custom_progress_status: widgets.HTML):\n",
    "    global dfloader, cached_donors, dataset_config\n",
    "    custom_progress_status.value = f\"Hotdeck starting, loading {len(donors)} donors...\"\n",
    "    cached_donors = dict()\n",
    "    sheet_name, column_name = dfloader.targets_index[0]\n",
    "    for filename in donors:\n",
    "        loader = DataFrameLoader.from_file(config[\"upload_dir\"] + filename, date_parser=dataset_config[\"date_parser\"])\n",
    "        loader.add_targets(column_name, sheet_name=sheet_name)\n",
    "        cached_donors[filename] = loader.df.dropna()\n",
    "\n",
    "\n",
    "def get_gap_boundaries(df: pd.DataFrame, gap_start_time: datetime, gap_end_time: datetime) -> tuple:\n",
    "    gap_start_idx = df.index.get_loc(gap_start_time) - 1\n",
    "    gap_end_idx = df.index.get_loc(gap_end_time) + 1\n",
    "\n",
    "    if gap_start_idx < 0:\n",
    "        gap_start_idx = 0\n",
    "    if gap_end_idx >= len(df):\n",
    "        gap_end_idx = len(df) - 1\n",
    "\n",
    "    return gap_start_idx, gap_end_idx\n",
    "\n",
    "\n",
    "def get_sampling_durations(receiver: pd.DataFrame, gap_start_idx: int, gap_end_idx: int, gap_start_time: datetime, gap_end_time: datetime) -> tuple:\n",
    "    global column_name\n",
    "    gap_duration = gap_end_time - gap_start_time\n",
    "    max_duration = gap_duration if gap_duration > timedelta(minutes=20) else timedelta(minutes=20)\n",
    "    index_count = len(receiver.index)\n",
    "\n",
    "    duration_before = timedelta(seconds=0)\n",
    "    while gap_start_idx > 0 \\\n",
    "            and duration_before < max_duration \\\n",
    "            and np.isnan(receiver[column_name][receiver.index[gap_start_idx]]) == False:\n",
    "        duration_before = gap_start_time - receiver.index[gap_start_idx]\n",
    "        gap_start_idx -= 1\n",
    "\n",
    "    duration_after = timedelta(seconds=0)\n",
    "    while gap_end_idx < index_count \\\n",
    "            and duration_after < max_duration \\\n",
    "            and np.isnan(receiver[column_name][receiver.index[gap_end_idx]]) == False:\n",
    "        duration_after = receiver.index[gap_end_idx] - gap_end_time\n",
    "        gap_end_idx += 1\n",
    "\n",
    "    return duration_before, duration_after\n",
    "\n",
    "\n",
    "def run_ui(gap_indices_count: int, custom_progress_status: widgets.HTML):\n",
    "    global gaps_done\n",
    "    start_time = datetime.now()\n",
    "    progress_bar = widgets.IntProgress(min=0, max=gap_indices_count, value=0)\n",
    "    display(progress_bar)\n",
    "    while gaps_done != progress_bar.max:\n",
    "        if not running:\n",
    "            return\n",
    "        display_progress(start_time, custom_progress_status, progress_bar)\n",
    "        time.sleep(0.5)\n",
    "    display_progress(start_time, custom_progress_status, progress_bar)\n",
    "    custom_progress_status.value = f\"Time taken: {(datetime.now() - start_time)}\"\n",
    "\n",
    "\n",
    "def display_progress(start_time: datetime, custom_progress_status: widgets.HTML, progress_bar) -> None:\n",
    "    global gaps_done\n",
    "    progress_bar.description = \"%d/%d: \" % (gaps_done, progress_bar.max)\n",
    "    progress_bar.value = gaps_done\n",
    "    elapsed = datetime.now() - start_time\n",
    "    total_time_estimate = (elapsed / (gaps_done + 1)) * progress_bar.max\n",
    "    eta = total_time_estimate - elapsed\n",
    "    eta -= timedelta(microseconds=eta.microseconds)\n",
    "    elapsed -= timedelta(microseconds=elapsed.microseconds)\n",
    "    custom_progress_status.value = f\"ETA: {eta} ({elapsed} elapsed)\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
